{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "(a) $\\alpha k_1 + k_2 \\quad \\forall \\alpha \\geq 0$ is a Kernel. Proof: if $v^T K_1 v \\geq 0$ and $v^T K_2 v \\geq 0 \\quad \\forall v \\neq 0 \\implies v^T K_1 v + v^T K_2 v \\geq 0 \\implies v^T (K_1 + K_2) v \\geq 0$ where $K_1$ and $K_2$ are $n \\times n$ matrices such that $[K_1]_{i,j} = k_1(x_i,x_j)$ and $[K_2]_{i,j} = k_2(x_i,x_j)$ for some $x_1,x_2,...,x_n \\in \\mathcal{X}$\n",
    "\n",
    "(b) $k_1 - k_2$ is not, in general, a kernel, because  $v^T K_1 v \\geq 0$ and $v^T K_2 v \\geq 0 \\not\\Rightarrow v^T (K_1-K_2) v \\geq 0$ \n",
    "\n",
    "(c) $f(x)k_1(x,y)f(y)$ is not, in general, a kernel because without further information we don't know when the sign of $f(x)$ and $f(y)$ will or will not be equal, so that positive definiteness of the resulting kernel matrix is not guaranteed.\n",
    "\n",
    "(d) $\\frac{k_1(x,y)}{\\sqrt{k_1(x,x) k_1(y,y)}}$ is a kernel, because is guaranteed to have a positive denominator. It is a scaled version of $k_1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "(a) predict a person's income from their social media network; predict traffic between cities from the network of flight connections\n",
    "\n",
    "(b) $\\Omega(f)^2 = \\frac{1}{2} \\sum_{i\\sim j}(f_i-f_j)^2 = f^TLf$. If we consider a simple graph with three nodes $\\{v_1,v_2,v_3\\}$ and $v_1\\sim v_2$, we would obtain the matrices:\n",
    "\n",
    "$$\n",
    "D=\\begin{bmatrix} 1&0&0 \\\\ 0&1&0 \\\\ 0&0&0\\end{bmatrix} \\quad A=\\begin{bmatrix} 0&1&0 \\\\ 1&0&0 \\\\ 0&0&0\\end{bmatrix} \\quad L = \\begin{bmatrix} 1&-1&0 \\\\ -1&1&0 \\\\ 0&0&0\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$f^TLf = (f_1-f_2)f_1 + (f_2-f_1)f_2 = f_1^2 - 2f_2f_1 + f_2^2$\n",
    "\n",
    "$\\frac{1}{2} \\sum_{i\\sim j}(f_i-f_j)^2 = \\frac{1}{2} (f_1-f_2)^2 + (f_2-f_1)^2 = f_1^2 - 2f_2f_1 + f_2^2$.\n",
    "\n",
    "Both calculations are equivalent. This generalizes to arbitrary matrices D, A, and L.\n",
    "\n",
    "(c) To show that $<f,g>_{\\mathcal{H}} = f^TLg$ is an inner product of the Hilbert space, we have to show that $<f,g>_{\\mathcal{H}} = <g,f>_{\\mathcal{H}} \\quad \\forall f,g \\in R^n$; that $a<f,g>_{\\mathcal{H}} = <af,g>_{\\mathcal{H}} \\quad \\forall a \\in \\mathbb{R}$; and that $<f,f>_{\\mathcal{H}} > 0 \\quad \\forall f \\in \\mathbb{R}^n$.\n",
    "\n",
    "$<f,g>_{\\mathcal{H}} = <g,f>_{\\mathcal{H}} \\quad \\forall f,g \\in R^n$ and $a<f,g>_{\\mathcal{H}} = <af,g>_{\\mathcal{H}} \\quad \\forall a \\in \\mathbb{R}$ always hold true for vector-matrix products with symmetric matrices.\n",
    "\n",
    "$<f,f>_{\\mathcal{H}} \\geq 0 \\quad \\forall f \\in \\mathbb{R}^n$ holds true if $L$ is positive semidefinite. As shown in (b), $f^TLf$ is equivalent to $\\Omega(f)$, which is a sum of squared expressions and can therefore not become negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "interpreter": {
   "hash": "9d6866e9f614e05fca89e1c72d21fa4e6410604c1bc12dd3d2df6b222f458453"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
