{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96d2211a5a01c5d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 5\n",
    "## Due May 23 at 12:00\n",
    "\n",
    "Please note: \n",
    "\n",
    "- Read the instructions in the exercise PDF and in this notebook carefully.\n",
    "- Add your solutions *only* at `YOUR CODE HERE`/`YOUR ANSWER HERE` and remove the corresponding `raise NotImplementedError()`.\n",
    "- Do not chance the provided code and text, if not stated.\n",
    "- Do not *add* or *delete* cells.\n",
    "- Do not `import` additional functionality. \n",
    "- Before submitting: Please make sure, that your notebook can be executed from top to bottom `Menu -> Kernel -> Restart & Run all`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "(a) First, the optimization problem has to be reformulated in standard form:\n",
    "\n",
    "minimize $f(x,y) = -x-y$\n",
    "\n",
    "s.t. $g(x,y) = x^2 + 2 y^2 -5 = 0$\n",
    "\n",
    "The Lagrangian is: $L(x,y,\\nu) = f(x,y) + \\nu g(x,y)$\n",
    "\n",
    "Next, set the partial derivatives of the Lagrangian to 0:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x} = 2\\nu x - 1 \\overset{!}{=} 0$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} = 4\\nu y - 1 \\overset{!}{=} 0$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\nu} = x^2 + 2 y^2 -5 \\overset{!}{=} 0$\n",
    "\n",
    "$\\implies x = 1.83; y = 0.91, \\nu = 0.27$\n",
    "\n",
    "Since the problem has an equality constraint, it must be active, since the optimal point must be on the surface $g(x) = 0$.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "(a) The dual problem can be formulated as:\n",
    "\n",
    "$\\max\\limits_{\\lambda_1,\\lambda_2} g(\\lambda_1,\\lambda_2)$ with $g(\\lambda_1,\\lambda_2) = \\inf \\limits_{x} L(x,\\lambda_1,\\lambda_2) ; \\quad L(x,\\lambda_1,\\lambda_2) = c^T x + \\lambda_1^T(Ax-b) + \\lambda_2^T (-x) = (c^T + \\lambda_1^TA-\\lambda_2^T)x - \\lambda_1^Tb; \\quad \\lambda_1 \\in \\mathbb{R}^m; \\lambda_2 \\in \\mathbb{R}^n$.\n",
    "\n",
    "At the infimum of x, the partial derivative of the Lagrangian $\\frac{\\partial L}{\\partial x} = (c^T + \\lambda_1^TA-\\lambda_2^T) = 0$. Hence the dual problem can be rewritten as:\n",
    "\n",
    "$\\max\\limits_{\\lambda_1, \\lambda_2} (c^T + \\lambda_1^TA-\\lambda_2^T)x - \\lambda_1^Tb $\n",
    "\n",
    "s.t. \n",
    "\n",
    "$\\lambda_1 \\geq 0$\n",
    "\n",
    "$\\lambda_2 \\geq 0$\n",
    "\n",
    "$(c^T + \\lambda_1^TA-\\lambda_2^T) = 0$\n",
    "\n",
    "which is again a linear program.\n",
    "\n",
    "\n",
    "(b) The Lagrangian of the quadratic program $L(x,\\lambda) = \\frac{1}{2} x^TQx + c^Tx + \\lambda (Ax-b)$ has the derivative $\\frac{\\partial L(x,\\lambda)}{\\partial x} = Qx + c + \\lambda A$. It can be solved for $x$ only if $Q$ is positive semidefinite, i.e., invertible. The problem in Exercise 1 a (trivial) quadratic program (assuming $\\mathbf{x} \\equiv (x,y)^T$), since the objective function can be written in the form $x^T Q x + c^T x$, where $Q = \\mathbf{0}$, which is a positive semidefinite matrix.\n",
    "\n",
    "(c) The dual function of a quadratic problem will be of the quadratic form $ (A\\lambda-b)^TQ^-1(A\\lambda-c) - \\lambda^Tb$. Since both the primal and the dual problem are quadratic functions, they will have one global extreme value, i.e., only one saddle point, which will be the solution to both the primal and dual problem, hence strong duality holds for quadratic programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "The resulting hyperplane is, by definition, in canonical form iff the smallest value of $Y_i(w^TX_i+b) = 1 =: Y^{\\circ}(w^TX^{\\circ}+b)$, hence we can ignore all other $(X_i,Y_i)$ pairs. For simplicity, we can assume (without loss of generality) that $sign(Y^{\\circ}) = 1$, hence the problem comes down to:\n",
    "\n",
    "$min \\frac{1}{2} ||w||^2 = \\frac{1}{2} w^T w$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$-w^TX^{\\circ}-b-1 \\leq 0$\n",
    "\n",
    "The Lagrangian is:\n",
    "\n",
    "$L = \\frac{1}{2} w^T w + \\lambda (-w^TX^{\\circ}-b-1)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = w+\\lambda X^{\\circ} = 0$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\lambda} = -w^TX^{\\circ}-b-1 = 0$\n",
    "\n",
    "$\\implies w^TX^{\\circ}+b = 1$, i.e., the solution will always be in canonical form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffb10fbc071fc814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.testing import assert_equal, assert_almost_equal\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#  Hide warnings of LinearSVC, LogisticRegression\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-65c9e677a5c1d059",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## a) Know the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0da6208813948939",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- **Features:** \n",
    "    Variables derived from some tissue imaging method, describing the size, shape, 'coastline' and texture of the tumors\n",
    "- **Labels:** \n",
    "    A binary categorization of the tumors into malignant and benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f454aa4ab0e7abc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'feature_names', 'filename', 'frame', 'target', 'target_names']\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "\n",
    "xs = dataset.data\n",
    "ys = dataset.target\n",
    "\n",
    "print(dir(dataset))\n",
    "print(dataset.feature_names)\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split `xs, ys` to the training set `xs_train, ys_train` (size $70\\%$) and the test set `xs_test, ys_test` (size $30\\%$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a440cbfbbb0a67b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## b)/c) Optimize the Hyperparameters: Linear SVM vs Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7d3b8a7e75f7895",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can do better than  0.03508771929824561\n",
      "(398, 30)\n"
     ]
    }
   ],
   "source": [
    "# center and normalize\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(xs)\n",
    "xs = scaler.transform(xs)\n",
    "\n",
    "# split into training and test test\n",
    "n_train = int(len(xs) * .7)\n",
    "\n",
    "xs_train, xs_test = xs[:n_train], xs[n_train:]\n",
    "ys_train, ys_test = ys[:n_train], ys[n_train:]\n",
    "\n",
    "# evaluate standard SVM\n",
    "svm_estimator = LinearSVC().fit(xs_train,ys_train)\n",
    "print('We can do better than ', np.mean(svm_estimator.predict(xs_test) != ys_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find good hyperparameters *without* the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0493355d0168522f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation errors of best model: Linear SVM 0.0251, Logistic Regression 0.0276\n"
     ]
    }
   ],
   "source": [
    "# DO NOT USE xs_test, ys_test here!\n",
    "# Please make your optimization reproducable (e.g. set random_state, seed, â€¦)\n",
    "# LinearSVC and LogisticRegression\n",
    "nfolds = 5\n",
    "split_idx = np.linspace(0, len(ys_train),nfolds+1).astype(int)\n",
    "cs = np.logspace(-5, 1, base=10, num = 100)\n",
    "cverror_svc = np.zeros((nfolds, len(cs)))\n",
    "cverror_lr = np.zeros((nfolds, len(cs)))\n",
    "samplecounter = np.arange(len(ys_train))\n",
    "for fold in range(nfolds):\n",
    "    for c_idx, c in enumerate(cs):\n",
    "        cv_selector = (samplecounter >= split_idx[fold]) & (samplecounter < split_idx[fold+1]) \n",
    "        train_selector  = ~cv_selector #invert cv bools to get training samples for current fold\n",
    "        \n",
    "        #Linear SVC\n",
    "        cverror_svc[fold, c_idx] = np.mean(LinearSVC(C=c).fit(xs_train[train_selector,:],ys_train[train_selector]).predict(xs_train[cv_selector,:]) != ys_train[cv_selector])\n",
    "\n",
    "        #same for logistic regression\n",
    "        cverror_lr[fold, c_idx] = np.mean(LogisticRegression(C=c).fit(xs_train[train_selector,:],ys_train[train_selector]).predict(xs_train[cv_selector,:]) != ys_train[cv_selector])\n",
    "\n",
    "cv_errors_svm = np.mean(cverror_svc,0)\n",
    "cv_errors_lr = np.mean(cverror_lr,0)\n",
    "best_params_svm, best_params_lr = {'C': cs[cv_errors_svm.argmin()]}, {'C': cs[cv_errors_lr.argmin()]}\n",
    "\n",
    "print(f\"Cross validation errors of best model: Linear SVM {cv_errors_svm.min():.4f}, Logistic Regression {cv_errors_lr.min():.4f}\")\n",
    "\n",
    "\n",
    "best_estimator_svm = LinearSVC(**best_params_svm).fit(xs_train,ys_train)\n",
    "best_estimator_lr = LogisticRegression(**best_params_lr).fit(xs_train,ys_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-213d9fdca48368f0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test errors of best model: Linear SVM 0.0117,     Logistic Regression 0.0292\n"
     ]
    }
   ],
   "source": [
    "# if you did not solve the cross validation, use this:\n",
    "# svm_estimator = LinearSVC(C=0.0035, random_state=42).fit(xs_train, ys_train)\n",
    "# lr_estimator = LogisticRegression(C=0.4037,random_state=42).fit(xs_train, ys_train)\n",
    "# else, use this:\n",
    "svm_estimator = best_estimator_svm\n",
    "lr_estimator = best_estimator_lr\n",
    "\n",
    "### YOUR CODE HERE\n",
    "print(f\"Test errors of best model: Linear SVM {np.mean(best_estimator_svm.predict(xs_test) != ys_test):.4f}, \\\n",
    "    Logistic Regression {np.mean(best_estimator_lr.predict(xs_test) != ys_test):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-563e6d1de9e41fbf",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "What do you observe and why?\n",
    "\n",
    "Answer: while the CV error is similar between both models (LR is a bit worse), the difference is more pronounced when it comes to the test set error. \n",
    "This could be related to the more robust algorithm in SVM, which allows it to generalize better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a7fd6a82e440e12c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## d) State concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5afbd54df2ee393d",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. **Ethical:** \n",
    "    When tuning the classifier, we did not differentiate between sensitivity and specificity, while in the breast cancer context, we will typically want to have high sensitivity if at the expense of specificity, because missing a true malignant case would be worse than a false alarm.\n",
    "\n",
    "2. **Technical/Statistical:**\n",
    "    we have used a fairly high number of features, some of which are clearly correlated (e.g. mean perimeter, mean radius). We should have checked if the feature space dimensionality can be reduced\n",
    "\n",
    "3. **Any:**\n",
    "    We have only considered linear classifiers."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "interpreter": {
   "hash": "c8de50c09079c2205839ee23bf826c226be6807115ed0be9e6dc25dbf1d3eba9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
